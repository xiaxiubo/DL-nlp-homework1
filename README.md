# DL-nlp-homework1
自然语言处理课程第一次作业

作业使用了金庸小说数据集，共16本。使用jieba分词对数据进行分词，计算了在词袋模型下中文词汇的平均信息熵。

main中执行上述操作，共分为三个子函数remove_punctuation()\book_read()\entropy_fig()

中文运行结果如下：

|词袋模型|按字|按词|
|:--|:--|:--|
|字/词总数|8764874|4316017|
|平均信息熵|9.078|12.141|
